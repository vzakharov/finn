Here’s the skinny on what folks are saying in the wild—mainly Reddit and Hacker News—about your target models. Spoiler: there’s no consensus, just enthusiasm and grumbling in roughly equal measure.

People love Jamba 1.6’s huge context window and RP chops, but it’s niche; Command A scores points for “free” API trials; DeepSeek V3 wows with reasoning transparency but some complain about cost and occasional glitches; Gemini 2.5 Pro shines at code but is repeatedly roasted for regressing on prose compared to its “Flash” sibling; Llama 4 Maverick earns praise for efficiency and rock-solid cost/perf; and GPT-4.1 is hailed as a “game-changer” if you don’t mind the hardware bill.

---

## Jamba 1.6

> “It beat Jamba 1.5 by a decent margin. About 21% more of our QA outputs were grounded correctly and it was basically tied with GPT-4o in how well …” ([Reddit][1])
> “It’s the best model I’ve tried so far for RP—blows everything out of the water … it feels almost like it was specifically trained for RP.” ([Reddit][2])

## Command A

> “Command a/command r can be used with a free trial for 1000 responses PER EMAIL! so basically limitless.” ([Reddit][3])

## DeepSeek V3

> “Deepseek with the right settings … can be very good, much better than anything else I’ve used, besides the hungriest of wallet eaters. It’s also very high in rankings for creative writing.” ([Reddit][3])
> “Deepseek is the state of the art right now in terms of performance and output. It’s really fast. The way it ‘explains’ how it’s thinking is remarkable.” ([Hacker News][4])
> “It’s not cheaper than Deepseek V3.1, though, and Deepseek outperforms on nearly everything. And only between 1–3× the throughput based on the openrouter metrics …” ([Hacker News][5])

## Gemini 2.5 Pro

> “I find Gemini 2.5 Pro to be slightly better at coding than Grok 3. However, Grok is not as good at understanding user intent which really brings the whole experience down for me.” ([Reddit][6])
> “THIS IS ABSURD! GEMINI 2.5 FLASH IS GIVING BETTER, MORE DETAILED, AND SMARTER ANSWERS THAN GEMINI 2.5 PRO … GEMINI 2.5 PRO IS LESS COMPETENT THAN GEMINI 2.5 FLASH ON TASKS THAT DON’T REQUIRE CODE. THIS IS OUTRAGEOUS!” ([Reddit][7])
> “After the update, the model appears to have regressed to 1.5 Pro’s level … the flash version performs better for text creation, which I find extremely frustrating :(” ([Reddit][7])

## Llama 4 Maverick

> “Llama 4 Maverick is the most efficient and provide better result than any LLM in the current time (again, judging from my agent project only).” ([Reddit][8])
> “Llama 4 is a base model, 2.5 Pro is a reasoning model, that’s just not a fair comparison.” ([Reddit][9])
> “Inference cost of Llama4 on openrouter is 10 times cheaper.” ([Reddit][8])

## GPT-4.1

> “I fine-tuned GPT 4.1 earlier today and achieved an ROC-AUC of 0.94. This is a game changer; it essentially ‘solves’ my particular class of problems. I have to get rid of an entire Llama-based RL pipeline I literally just built over the past month.” ([Reddit][10])

---

No holy grail here—just a buffet of strengths, weaknesses, and fiercely argued turf wars. Pick your poison (or birthday cake).

[1]: https://www.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/?utm_source=chatgpt.com "Jamba mini 1.6 actually outperformed GPT-40 for our RAG support bot"
[2]: https://www.reddit.com/r/SillyTavernAI/comments/1jd8d1t/dont_sleep_on_ai21_jamba_16_large/?utm_source=chatgpt.com "Don't sleep on AI21: Jamba 1.6 Large : r/SillyTavernAI - Reddit"
[3]: https://www.reddit.com/r/SillyTavernAI/comments/1jmb0fk/just_got_safety_filters_from_anthropic_i_need/ "Just got safety filters from Anthropic, I need alternatives to Claude Sonnet. : r/SillyTavernAI"
[4]: https://news.ycombinator.com/item?id=42890709 "What is the comparison of this versus DeepSeek in terms of good results and cost... | Hacker News"
[5]: https://news.ycombinator.com/item?id=43920132 "It's not cheaper than Deepseek V3.1, though, and Deepseek outperforms on nearly ... | Hacker News"
[6]: https://www.reddit.com/r/ArtificialInteligence/comments/1jyizj3/gemini_25_pro_is_by_far_my_favourite_coding_model/?utm_source=chatgpt.com "Gemini 2.5 Pro is by far my favourite coding model right now - Reddit"
[7]: https://www.reddit.com/r/Bard/comments/1kk00zr/google_what_have_you_done_to_gemini_25_pro/ "GOOGLE, WHAT HAVE YOU DONE TO GEMINI 2.5 PRO?! : r/Bard"
[8]: https://www.reddit.com/r/LLMDevs/comments/1jzhc3x/llama_4_received_so_much_hate_but_it_actually/ "Llama 4 received so much hate but it actually performs better than newly released GPT 4.1 in my workflow. : r/LLMDevs"
[9]: https://www.reddit.com/r/singularity/comments/1jscj37/llama_4_vs_gemini_25_pro_benchmarks/ "Llama 4 vs Gemini 2.5 Pro (Benchmarks) : r/singularity"
[10]: https://www.reddit.com/r/LocalLLaMA/comments/1k25suh/gpt_41_is_a_game_changer/ "GPT 4.1 is a game changer : r/LocalLLaMA"