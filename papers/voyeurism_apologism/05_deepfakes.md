## Deepfakes: The New Face of Voyeurism

**Deepfakes** – AI-generated synthetic videos or images that swap a person’s likeness into footage they never actually participated in – emerged in the late 2010s as a potent new form of voyeuristic exploitation. Almost immediately, the technology was used to create fake pornography, almost always targeting women (often celebrities at first, and later private individuals). Like traditional voyeurism, deepfake porn involves viewing someone in sexual situations *without their consent*. The difference is that instead of *observing* a real event, the perpetrator is *fabricating* an event – but the goal is the same: to see (and share) an intimate image of a person who never agreed to be seen in that way. It didn’t take long for scholars and activists to point out the parallel. In 2024, legal experts dubbed deepfake sexual imagery the **“new voyeurism,”** arguing that creating a fake explicit image of someone *“is the new form of ‘taking’ intimate images without consent”* and should be treated legally akin to conventional voyeurism. In other words, the lack of *real* camera or *actual* peeping is a semantic distinction – morally and emotionally, the *effect on the victim* and the *intent of the perpetrator* align with age-old voyeurism.

What’s striking is how **the same rhetorical devices used to excuse old-school voyeurism have resurfaced to excuse deepfakes**. Below are some of the common apologetic refrains, unchanged in spirit despite the high-tech gloss:

* **“No Harm Done if No One Knows” – Privacy Is Intact Until Discovery:**  Just as voyeurs have long claimed their peeping hurts no one if the victim remains unaware, deepfake defenders argue that a privately created deepfake that isn’t widely circulated is *victimless*. They posit scenarios where an individual secretly makes a pornographic deepfake of someone *“for personal use only,”* suggesting that if the person depicted never finds out, then it’s as if nothing happened. This rationale willfully ignores the violation of consent that exists *regardless* of awareness. It also fails to consider the risk – as with any digital file – that the fake could later leak or be discovered. Nevertheless, the allure of the *“perfect crime”* (enjoying the image of someone without them ever knowing) remains a cornerstone of voyeuristic fantasy and is often used to downplay deepfakes: *It’s only a private fantasy, not a real assault*.

* **“It’s Just Fantasy / Curiosity – We All Wonder What So-and-So Looks Like”:**  Many deepfakes target famous actresses, singers, or even one’s own acquaintances. A common excuse is that these creations are merely an extension of *fantasy*. *Everyone imagines celebrities naked,* apologists say, *the deepfake is just an artistic (or technological) expression of that private fantasy*. This echoes historical apologetics where peeping was cast as *natural male curiosity*. During the Victorian era, some argued that adolescent boys peeping on bathing females were just acting out “healthy” curiosity about the opposite sex. Similarly, today’s defenders might say viewing an AI-generated nude of a classmate or co-worker is a form of “innocent” exploration – *a high-tech peep* that shouldn’t be conflated with physical harm. By labeling the act harmless fantasy, they seek to absolve themselves of the **power imbalance** at play (since the subject is not an equal participant in this “fantasy”).

* **“No Physical Contact, So It’s Not a Big Deal”:**  This is a direct carryover from how people diminished old voyeurism. Voyeurism and deepfakes are *“only images”* or *“only looking,”* not physical molestation – so, the argument goes, they should not be treated with the seriousness reserved for “real” sexual crimes. Indeed, voyeurs historically benefited from this false hierarchy, and deepfake creators attempt the same. However, modern legal thought is pushing back, noting that image-based sexual abuse can be deeply traumatic even without physical contact. Victims of deepfake porn often report feelings of **sexual violation and helplessness comparable to an assault**, because their likeness – an extension of their person – was used in a sexual way against their will. Nonetheless, the minimization persists in online debates: if it’s “just pixels” or “just code,” certain people insist it can’t be that harmful – a claim belied by the lived experiences of those targeted.

* **“She Shouldn’t Have Posted Pics / It’s Her Fault for Being Attractive”** (Victim-Blaming):  In a twist on the “assumed the risk” notion, some justify deepfakes by pointing to the images the victim *did* share. For example, if a young woman has selfies on her social media, deepfake creators feel *entitled* to those images as fodder. *She put them out there, I just used them.* This parallels blaming a voyeurism victim for, say, wearing revealing clothes or not covering her windows – *she made it too easy*. It’s a deeply entrenched apologetic tactic to shift responsibility from the violator to the victim’s mere existence or normal behavior. In South Korea’s *molka* debates, there have been instances of men claiming that women’s fashion (skirts, etc.) tempted voyeurs, an argument swiftly rejected by women’s rights groups as absurd. Yet with deepfakes, because the offending “material” is often innocuous photos that *the victim herself posted*, perpetrators latch onto that as a fig leaf of consent – which it absolutely is not. **Consent to share a normal photo is not consent to have one’s nude body fabricated**, but this nuance is lost on those determined to excuse their violations.

* **“Everyone Does It (Secretly)” & Peer Encouragement:**  On certain forums where non-consensual deepfakes are traded, there’s a sense of community that normalizes the practice. Users egg each other on, sharing tips and results, reinforcing the idea that *this is just a new hobby that lots of people do*. The behavior is cocooned in group affirmation. Historically, voyeurs also found camaraderie – the proverbial *boys’ club* where sharing stolen glances or illicit photos was met with high-fives instead of condemnation. This social normalization is a rhetorical strategy in that it reduces individual accountability: *if many are doing it, it can’t be that wrong – it’s society’s norm that’s changing.* Indeed, one of the dangers noted by ethicists is how deepfakes at scale could further **normalize the non-consensual consumption of someone’s intimate likeness**, making it seem routine.

It’s evident that **deepfake apologism is not novel** – it’s recycling scripts from the voyeur’s playbook. What’s novel is the *power and reach* of the technology. A 19th-century peeper could at most violate one person at a time and risk being seen; a deepfaker with minimal skill can potentially create explicit images of dozens of women and distribute them globally, all while remaining anonymous. The potential for scaled, anonymous abuse makes these apologetic narratives even more perilous. If believed, they could stall urgently needed interventions by painting deepfakes as *personal, harmless indulgences* rather than the far-reaching sexual violations they truly are. As law professor Clare McGlynn argues, **deepfake sexual abuse sits on the same continuum as voyeurism and other image-based sexual crimes** – it is part of *“the cultural scaffolding of rape culture,” undermining the boundaries of consent and normalizing violations in the digital age*. In plainer terms, each excuse for deepfakes borrows from a culture that long trivialized men’s invasive gaze, and in doing so, it props up a system in which (mostly) women’s consent can be overridden for (mostly) men’s pleasure.